{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Tensorflow Code for a Convolutional Neural Network\n",
    "<br>\n",
    "from: http://www.jessicayung.com/explaining-tensorflow-code-for-a-convolutional-neural-network/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input]) # input, i.e. pixels that constitute the image\n",
    "y = tf.placeholder(tf.float32, [None, n_classes]) # labels, i.e which digit the image is\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d93809139ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Construct model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-913e6d5dccd5>\u001b[0m in \u001b[0;36mconv_net\u001b[0;34m(x, weights, biases, dropout)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Convolution Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wc1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bc1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Max Pooling (down-sampling)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxpool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv2d' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Store layers weight &amp; bias\n",
    "weights = {\n",
    "# 5x5 conv, 1 input, 32 outputs\n",
    "'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "# 5x5 conv, 32 inputs, 64 outputs\n",
    "'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "# fully connected, 7*7*64 inputs, 1024 outputs\n",
    "'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "# 1024 inputs, 10 outputs (class prediction)\n",
    "'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "'bc1': tf.Variable(tf.random_normal([32])),\n",
    "'bc2': tf.Variable(tf.random_normal([64])),\n",
    "'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Translating the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Let’s draw the model the function conv_net represents. <u> The batch size given is 128. That means that each time, at most 128 images are fed into our model.</u>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The big picture:\"\n",
    "\n",
    "<img src='../conv-big-pic.png' style=\"max-width:25%;width:auto;height:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"../imgs/cnn-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/cnn-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We can see that there are fives types of layers here:\n",
    "<ul>\n",
    "    <li>convolution layers,\n",
    "    <li>max pooling layers,\n",
    "    <li>layers for reshaping input,\n",
    "    <li>fully-connected layers and\n",
    "    <li>dropout layers.\n",
    "</ul>\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What is conv2d (convolution layer)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A convolution layer tries to extract higher-level features by replacing data for each (one) pixel with a value computed from the pixels covered by the e.g. 5×5 filter centered on that pixel(all the pixels in that region).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/cnn-fletcher-bach.gif\" style=\"max-width:25%;width:auto;height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We slide the filter across the width and height of the input and compute the dot products between the entries of the filter and input at each position. I explain this further when discussing tf.nn.conv2d() below.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function comprises three parts:\n",
    "\n",
    "<ol> \n",
    "    <li> Conv2D layer from Tensorflow </li>\n",
    "            <ol>\n",
    "                <li> tf.nn.conv2d() </li>\n",
    "                <li> This is analogous to xW (multiplying input by weights) in a fully connected layer. </li>\n",
    "            </ol>\n",
    "    <li> Add bias </li>\n",
    "    <li> ReLU activation </li>\n",
    "        <ol>\n",
    "            <li> This transforms the output like so:ReLU(x) = max(0,x). (See previous post for more details). </li>\n",
    "        </ol>\n",
    "</ol>\n",
    "\n",
    "\n",
    "<img src=\"../imgs/relu.png\">\n",
    "\n",
    "\"You can see it is structurally the same as a fully connected layer, except we multiply the input with weights in a different way.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2D layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The key part here is tf.nn.conv2d(). Let’s look at each of its arguments.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"x is the input.\n",
    "W are the weights.\n",
    "\n",
    "The weights have four dimensions:  [filter_height, filter_width, input_depth, output_depth]. What this means is that we have  output_depth filters in this layer. Each filter considers information with dimensions [filter_height, filter_width, input_depth] at a time. Yes, each filter goes through ALL the input depth layers. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> x is the input. </li>\n",
    "    <li> W are the weights </li>\n",
    "        <ul>\n",
    "            <li> The weights have four dimensions:  [filter_height, filter_width, input_depth, output_depth] </li>\n",
    "            <li> What this means is that we have  output_depth filters in this layer. </li>\n",
    "                <ul> \n",
    "                    <li> Each filter considers information with dimensions [filter_height, filter_width, input_depth] at a time. Yes, each filter goes through ALL the input depth layers. </li>\n",
    "                    <li> <img src=\"../imgs/3d-cnn.png\">\n",
    "                    <li>This is like how, in a fully connected layer, we may have ten neurons, each of which interacts with all the neurons in the previous layer.</li>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
